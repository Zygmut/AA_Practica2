{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2\n",
    "\n",
    "_Autores_\n",
    "\n",
    "-   Palmer Pérez, Rubén\n",
    "\n",
    "## Descipción de la practica\n",
    "\n",
    "Este proyecto se basa en el análisis del conjunto de datos [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), abordando diversas problemáticas utilizando [Convolutional Neural Networks (CNN)](<(https://en.wikipedia.org/wiki/Convolutional_neural_network)>).\n",
    "\n",
    "### Tareas asignadas\n",
    "\n",
    "1. Clasificación Perros vs. Gatos:\n",
    "\n",
    "    - Implementación de clasificación para distinguir entre perros y gatos.\n",
    "    - Utilización de una red creada manualmente y otra preentrenada.\n",
    "\n",
    "2. Clasificación de la Raza:\n",
    "\n",
    "    - Clasificar la raza de los animales presentes en el conjunto de datos.\n",
    "\n",
    "3. Detección de la Posición de la Cabeza:\n",
    "\n",
    "    - Investigación y aplicación de técnicas para identificar la posición de la cabeza en las imágenes.\n",
    "\n",
    "4. Segmentación del Animal:\n",
    "\n",
    "    - Implementación de un sistema para segmentar el objeto animal en las imágenes proporcionadas.\n",
    "\n",
    "En este documento se examina el proceso de aprendizaje, procesamiento de datos y pruebas que han llevado a la solución de los dos primeros puntos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paquetes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from os import path, listdir\n",
    "from torchvision import transforms\n",
    "from torchview import draw_graph\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import filters, img_as_ubyte, color, img_as_float\n",
    "from skimage.transform import rotate\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.util import random_noise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import reduce\n",
    "from operator import sub\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from CatDogDataset import CatDogDataset\n",
    "from network import CatDogNet, CatDogResnet34, CatDogResnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"IMAGE_SIZE\": (128, 128),\n",
    "    \"DATA_PATH\": path.join(path.abspath(path.pardir), \"data\", \"images\"),\n",
    "    \"SEED\": 27,\n",
    "    \"MODEL\": {\"EPOCH\": 15, \"LEARNING_RATE\": 0.00001},\n",
    "}\n",
    "\n",
    "random.seed(CONFIG.get(\"SEED\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, eliminaremos posibles artefactos de previas ejecuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_remove = (\n",
    "    os.path.join(folder, file)\n",
    "    for folder, _, files in os.walk(path.join(CONFIG.get(\"DATA_PATH\")))\n",
    "    for file in files\n",
    "    if \"generated\" in file\n",
    ")\n",
    "\n",
    "any(map(os.remove, files_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "El conjunto de datos establece lo siguiente en los documentos adjuntos de etiquetado\n",
    "\n",
    "_Todas las imágenes con la 1ª letra en mayúscula son imágenes de gatos, mientras que las imágenes con la primera letra en minúscula son imágenes de perros._\n",
    "\n",
    "Gracias a que el nombre del archivo tambien indica la raza del animal, utilizaremos el nombre de cada imagen para generar un `dataframe` que contenga el `path` de cada imagen y las etiquetas necesarias. Adicionalmente comentar que, aunque se podría directamente codificar las etiquetas para su futuro uso, se mantendrá el uso de etiquetas con una mayor carga semántica; evitando los [números mágicos](<https://en.wikipedia.org/wiki/Magic_number_(programming)>) fuera del estudio de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [\n",
    "    path.join(CONFIG.get(\"DATA_PATH\"), file)\n",
    "    for file in listdir(CONFIG.get(\"DATA_PATH\"))\n",
    "    if file.endswith(\".jpg\")\n",
    "]\n",
    "\n",
    "labels = pd.DataFrame(\n",
    "    {\n",
    "        \"image_name\": image_files,\n",
    "        \"species\": [\n",
    "            \"cat\" if path.basename(file)[0].isupper() else \"dog\" for file in image_files\n",
    "        ],\n",
    "        \"breed\": [\n",
    "            \"_\".join(path.basename(file).split(\"_\")[:-1]).lower()\n",
    "            for file in image_files\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentar que, explorando los diferentes tipos de imagenes manualmente, se han encontrado diferentes extensiones. Con el objetivo de simplificar el proceso de procesamiento de imagenes, solo se han tenido en cuenta las imagenes con extensión `.jpg`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis\n",
    "\n",
    "En este apartado, tiene como objetivo examinar las etiquetas e imágenes antes de proceder a su carga. Se busca, especialmente, la identificación de patrones y desbalancesen las clasificaciones. Para ello, se usara un conjunto de calculos imperativos y exploración grafica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    labels[\"species\"].values,\n",
    ")\n",
    "plt.title(\"Species frequency distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La visualización revela un notable desbalance entre las especies, viendo que hay aproximadamente el doble de perros en comparación con los gatos. En este escenario, nos hemos planteado tres posibles soluciones:\n",
    "\n",
    "-   **Permanecer en este estado:** Existe el riesgo de que el modelo realice predicciones erróneas basñandose únicamente en la cantidad de datos. Por ejemplo, en una situación extrema, si hay 1 gato y 99 perros, el modelo etiquetaría todas las imágenes como perros, ya que tiene un 99% de probabilidades de acertar.\n",
    "-   **Downsample**: Eliminar aleatoriamente observaciones de los perros hasta igualar el número de gatos; posiblemente reduciendo la precisión de acierto ante las predicciones de los perros\n",
    "-   **Upsample**: Emplear técnicas de [aumento de datos](https://en.wikipedia.org/wiki/Data_augmentation) para generar nuevas imágenes de gatos hasta igualar la el número de perros; posiblemente creando `overfitting`\n",
    "\n",
    "En este especifico caso, y con fines academicos, se ha optado por generar imágenes adicionales de gatos\n",
    "\n",
    "Asi pues, procedemos a la distribución de razas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=labels, x=\"breed\", hue=\"species\")\n",
    "plt.title(\"Breed frequency distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de razas se mantiene generalmente estable, a excepción de `stanffordshire_bull_terrier` y `scotish_terrier`, las cuales tienen ligeramente menos datos. Así pues, no será necesario realizar modificaciones en los datos en relación a la especie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gatos VS Perros: Clasificación\n",
    "\n",
    "### Aumento de datos\n",
    "\n",
    "Como se ha mencionado previamente, la falta de observaciones de gatos genera un desbalance de fequencia. Para mitigar dicho impacto, se generarán nuevas imágenes de gatos mediante la aplicación de un conjunto aleatorio de transformaciones. Al concluir este proceso, se espera obtener un conjunto adicional de imágenes de gatos que, idealmente, sean suficientemente diveras como para reducir la posibilidad que el modelo sufra de `overfitting` o al menos reducir su efecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulations = [\n",
    "    lambda img: rotate(img, random.randint(0, 360), mode=\"wrap\"),\n",
    "    lambda img: random_noise(img, seed=CONFIG.get(\"SEED\"), mode=\"s&p\", amount=0.1),\n",
    "    lambda img: random_noise(img, seed=CONFIG.get(\"SEED\"), mode=\"gaussian\", var=0.01),\n",
    "    lambda img: img[::-1, ::-1],\n",
    "    lambda img: filters.gaussian(img, sigma=1),\n",
    "]\n",
    "\n",
    "PROPORTIONAL_MANIPULATION_AMOUNT = 2\n",
    "\n",
    "cat_data = labels[labels[\"species\"] == \"cat\"][\"image_name\"].to_list()\n",
    "\n",
    "amount_of_cats = abs(sub(*labels[\"species\"].value_counts().values))\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(amount_of_cats):\n",
    "    random_manipulations = random.sample(\n",
    "        manipulations,\n",
    "        random.randint(1, len(manipulations) // PROPORTIONAL_MANIPULATION_AMOUNT),\n",
    "    )\n",
    "\n",
    "    choice = random.choice(cat_data)\n",
    "    img = reduce(\n",
    "        lambda img, manipulation: manipulation(img),\n",
    "        random_manipulations,\n",
    "        imread(choice),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        imsave(\n",
    "            path.join(path.join(CONFIG.get(\"DATA_PATH\")), f\"generated{i}.jpg\"),\n",
    "            img_as_ubyte(img),\n",
    "        )\n",
    "    except:\n",
    "        print(\"Had some error with image\", choice, \"skiping...\")\n",
    "        continue\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"image_name\": path.join(\n",
    "                path.join(CONFIG.get(\"DATA_PATH\")), f\"generated{i}.jpg\"\n",
    "            ),\n",
    "            \"species\": \"cat\",\n",
    "            \"breed\": \"_\".join(path.basename(choice).split(\"_\")[:-1]).lower(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "labels_cat_dogs = pd.concat([labels, pd.DataFrame(records)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    labels_cat_dogs[\"species\"].values,\n",
    ")\n",
    "plt.title(\"Species frequency distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pues, tenemos un conjunto de datos balanceados con respecto a la especie de los animales.\n",
    "\n",
    "### `Finetunning` de la media y la desviación estandar\n",
    "\n",
    "Durante la aplicación de la transformaciones, es común normalizar los datos de entrada para redulir la posible influencia de rrores provenientes de valores extremos. Comunmente se usan los valores expuestos por [ImageNet](https://image-net.org/) que son los siguientes:\n",
    "\n",
    "-   media: [0.485, 0.456, 0.406]\n",
    "-   std: [0.229, 0.224, 0.225]\n",
    "\n",
    "No obstante, para una mejor adaptación de los datos, se calcularán dichos valores para el conjunto de datos a transformar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(paths: list[str]) -> tuple[int, int]:\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "\n",
    "    for path in paths:\n",
    "        try:\n",
    "            img = imread(path)\n",
    "            img = img_as_float(img)\n",
    "            if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
    "                img = color.gray2rgb(img)\n",
    "            img = img.transpose((2, 0, 1))\n",
    "            img = img.reshape(3, -1)\n",
    "            mean += img.mean(axis=1)\n",
    "            std += img.std(axis=1)\n",
    "        except:\n",
    "            print(\"failure on image\", path, \" skiping...\")\n",
    "            continue\n",
    "\n",
    "    mean /= len(paths)\n",
    "    std /= len(paths)\n",
    "\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "\n",
    "norm_data = calculate_mean_std(labels_cat_dogs[\"image_name\"])\n",
    "print(norm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados, extremadamente similares a los de ImageNet, refuerzan la idea de que nos encontramos en la proximidad al domminio de resultados para imágenes genéricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(CONFIG.get(\"IMAGE_SIZE\")),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*norm_data),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de las etiquetas\n",
    "\n",
    "Como ya se mencionó previamante, la etiquetas que actualmente poseemos están cargadas semánticamente. Sin embargo, para ofrecerlas como `target` para el modelo, será necesaria codificarlas numericamente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_encoder = LabelEncoder()\n",
    "labels_cat_dogs[\"encoded_species\"] = species_encoder.fit_transform(\n",
    "    labels_cat_dogs[\"species\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El uso de un `LabelEncoder` posibilita un mejor entendimiento de las predicciones realizadas por el modelo, ya que las etiquetas codificadas numéricamente pueden ser revertidas al formato original de manera sencilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento, validación y prueba\n",
    "\n",
    "En este escenario, implementaremos una división de los datos utilizando la siguiente proporción:\n",
    "\n",
    "-   70% para entrenamiento\n",
    "-   15% para validación\n",
    "-   15% para prueba\n",
    "\n",
    "Según se sugiere en la fuente [aquí](https://www.v7labs.com/blog/train-validation-test-set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_data, train_data, rest_pred, train_pred = train_test_split(\n",
    "    labels_cat_dogs[\"image_name\"].values,\n",
    "    labels_cat_dogs[\"encoded_species\"].values,\n",
    "    test_size=0.7,\n",
    "    random_state=CONFIG.get(\"SEED\"),\n",
    ")\n",
    "\n",
    "val_data, test_data, val_pred, test_pred = train_test_split(\n",
    "    rest_data, rest_pred, test_size=0.5, random_state=CONFIG.get(\"SEED\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se ha creado una implementación personalizada basada en el modulo [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) de Pytorch, permitiendonos un uso sencillo e intuitivo de los `DataLoader`, encargados de ofrecer los datos al modelo aplicando las transformaciones especificadas previamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(CatDogDataset(train_data, train_pred, transform), batch_size=100)\n",
    "val_dl = DataLoader(CatDogDataset(val_data, val_pred, transform), batch_size=100)\n",
    "test_dl = DataLoader(CatDogDataset(test_data, test_pred, transform), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del modelo\n",
    "\n",
    "La CNN diseñada tiene como objetivo la clasificación binaria entre gatos y perros. Para ello se ha implementado la siguiente arquitectura:\n",
    "\n",
    "-   **Bloque convolucional**: Un conjunto de bloques que aplican `Convolution2D`, `batch normalization`, `ReLU` y `MaxPool2D`. Este bloque tiene como objetivo realizar convoluciones para extraer características relevantes, reduciendo al máximo el ruido y la cantidad de parámetros.\n",
    "-   **Flatten**: Transormando la matriz resultante en un tensor 1-dimensional\n",
    "-   **Linear**: Primera `fully connected` capa. Debido a que se han usado 4 capas de dropout, es necesario multiplicar las salidas de la anterior capa (512) por 8^2.\n",
    "-   **ReLU**: Eliminando posibles valores negativos en las salidas\n",
    "-   **Dropout**: Para reducir la posibilidad de `overfitting` con el tradeback de posiblemente no llegar a su maximo nivel de predicción\n",
    "-   **Linear**: La última `fully connected` capa, encargada de proporcionar la salida final de la red. El número de nodos en esta capa es igual al número de clases. En el caso de la clasificación binaria serían 2\n",
    "\n",
    "A continuación se podrá ver la representación grafica de la arquitectura usando [torchview](https://github.com/mert-kurttutan/torchview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogNet(2), input_size=(1, 3, 128, 128), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Finalmente, entrenaremos el modelo un total de 15 epochs. Adicionalmente, y con el objetivo de apreciar si el modelo ha sufrido de overfitting, se visualizará las `time series` del `accuracy` del entrenamiento y la validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs, train_loader, validation_loader):\n",
    "    model.train()\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for data, label in train_loader:\n",
    "            data = data.to(CONFIG.get(\"DEVICE\"))\n",
    "            label = label.to(CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_accuracy += acc / len(train_loader)\n",
    "            epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        train_acc.append(epoch_accuracy)\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(\n",
    "            \"Epoch : {}, train accuracy : {}, train loss : {}\".format(\n",
    "                epoch + 1, epoch_accuracy, epoch_loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in validation_loader:\n",
    "                data = data.to(CONFIG.get(\"DEVICE\"))\n",
    "                label = label.to(CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "                # No optimizer as we don't want to change the weights\n",
    "\n",
    "                acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "                epoch_val_accuracy += acc / len(validation_loader)\n",
    "                epoch_val_loss += val_loss / len(validation_loader)\n",
    "\n",
    "            valid_acc.append(epoch_val_accuracy)\n",
    "            valid_loss.append(epoch_val_loss)\n",
    "            print(\n",
    "                \"Epoch : {}, val_accuracy : {}, val_loss : {}\".format(\n",
    "                    epoch + 1, epoch_val_accuracy, epoch_val_loss\n",
    "                )\n",
    "            )\n",
    "    model.eval()\n",
    "    return list(map(float, train_acc)), list(map(float, valid_acc)), list(map(float, train_loss)), list(map(float, valid_loss))\n",
    "\n",
    "\n",
    "def train_val_plot(train, val, metric):\n",
    "    plt.plot(range(len(train)), train, label=\"Entrenamiento\")\n",
    "    plt.plot(range(len(val)), val, label=\"Validación\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\n",
    "        f\"{metric} del conjunto de entrenamiento y validación con respecto al tiempo\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = CatDogNet(2)\n",
    "\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\"))\n",
    "    )\n",
    "else:\n",
    "    model = model.to(CONFIG.get(\"DEVICE\"))\n",
    "    train_acc, valid_acc, train_loss, valid_loss= train(\n",
    "        model=model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=model.parameters(), lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\")\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=train_dl,\n",
    "        validation_loader=val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(train_acc, valid_acc, \"Precisión\")\n",
    "    train_val_plot(train_loss, valid_loss, \"Perdida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis muestra que, con el tiempo, el conjunto de validación experimenta una disminución en el incremento del `accuracy`, posiblemente indicando que el modelo está empezando a experimentar `overfitting`. Incrementar el tamaño del conjunto de datos podría ayudar a mitigar este problema. A continuación, se evaluará la eficacia de las predicciones utilizando el conjunto de pruebas; experimentando practicamente si el modelo ha conseguido generalizar la idea de gato y perro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene un gran numero de parametros, lo que posiblemente afectaba al tiempo de procesamiento de cada uno de los batches. Adicionalmente comentar que, si eliminasemos algunos `BatchNorm2d` de los convolution blocks definidos, el modelo reduciría considerablemente su tiempo de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    overall_acc = 0\n",
    "    for image, label in test_loader:\n",
    "        image, label = [data.to(device) for data in (image, label)]\n",
    "\n",
    "        test_output = model(image).argmax(dim=1)\n",
    "        predictions.extend(list(map(int, test_output)))\n",
    "        real_labels.extend(list(map(int, label)))\n",
    "        acc = float((test_output == label).float().mean())\n",
    "        overall_acc += acc / len(test_dl)\n",
    "\n",
    "    print(overall_acc)\n",
    "\n",
    "    cm = confusion_matrix(real_labels, predictions)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(13, 13))\n",
    "    disp.plot(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test(model, test_dl, CONFIG.get(\"DEVICE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a la matriz de correlación, podemos observar que el modelo tiende a fallar con mayor frecuencia al predecir las imágenes de los gatos. Esto podría ser causado por las transformaciones aplicadas para generar datos sintéticos; siendo estas no suficientemente diversas en comparación con las características de los gatos reales, posiblemente introduciendo un ruido que incapacitaba al modelo a generalizar de manera efectiva dichas características. Esta hipótesis se refuerza al notar que la clasificación de los perros es prácticamente impecable, con solo 61 predicciones erróneas en contraste con los 256 fallos en la predicción de gatos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, data_loader, label_encoder, n_images=4):\n",
    "    test_cat_dog_pairs = list(zip(*next(iter(data_loader))))\n",
    "\n",
    "    random_images = random.sample(test_cat_dog_pairs, n_images)\n",
    "\n",
    "    _, axs = plt.subplots(1, n_images, figsize=(15, 10))\n",
    "\n",
    "    for i, (image, label) in enumerate(random_images):\n",
    "        prediction = model(torch.unsqueeze(image, 0)).argmax(dim=1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "\n",
    "        if image.shape[0] == 1:\n",
    "            image = np.repeat(image, 3, axis=0)\n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "        axs[i].imshow(np.transpose(image, (1, 2, 0)), vmin=0, vmax=1)\n",
    "        axs[i].set_title(\n",
    "            f\"Real: {label_encoder.inverse_transform([label])[0]}\\nPrediction: {label_encoder.inverse_transform(prediction)[0]}\"\n",
    "        )\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_predictions(model, test_dl, species_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet34\n",
    "\n",
    "Se usara el modelo [Resnet34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html) para comparar los anteriores resultados. Se entrenará con los mismos datos, transformaciones, criterio y optimizador por lo que, esencialmente, solo estamos cambiando el modelo usado. Para solucionar el problema de dimensionalidad del modelo original, se ha eliminado la ultima capa y remplazado por la siguiente estructura:\n",
    "\n",
    "-   Flatten()\n",
    "-   Linear(512, 512)\n",
    "-   ReLU()\n",
    "-   Dropout(0.5)\n",
    "-   Linear(512, num_classes)\n",
    "\n",
    "En este caso el `num_classes` es 2 ya que es un problema de clasificación binaria. Esta pues es la estructura final:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogResnet34(2), input_size=(1, 3, 256, 256), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos el modelo. Comentar que, debido a que usamos los pesos originales de resnet34, el proceso de entrenamiento será significativamente más rápido de lo esperado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34_model = CatDogResnet34(2)\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    resnet34_model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\"))\n",
    "    )\n",
    "else:\n",
    "    resnet34_model = resnet34_model.to(CONFIG.get(\"DEVICE\"))\n",
    "    resnet34_train_acc, resnet34_valid_acc, resnet34_train_loss, resnet34_valid_loss = train(\n",
    "        model=resnet34_model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=resnet34_model.parameters(),\n",
    "            lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\"),\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=train_dl,\n",
    "        validation_loader=val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        resnet34_model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(resnet34_train_acc, resnet34_valid_acc, \"Precisión\")\n",
    "    train_val_plot(resnet34_train_loss, resnet34_valid_loss, \"Perdida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos apreciar como a partir del 4 `epoch`, el modelo no ha sufrido de un cambio en las precisiones, por lo que se podría haber acabado el proceso de entrenamiento antes de tiempo. Al tener mas capas convolucionales y [skip connections](https://www.analyticsvidhya.com/blog/2021/08/all-you-need-to-know-about-skip-connections/) aumentando el rendimiento de los modelos. Las `skip connections` permiten que la información fluya más fácilmente a través de las capas, mitigando problemas como el desvanecimiento del gradiente y facilitando el entrenamiento de modelos más profundos; resultando en una convergencia más rápida y eficiente durante el entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(resnet34_model, test_dl, CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "show_predictions(resnet34_model, test_dl, species_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fallando un total de 32 veces, el modelo tiene una precisión del 98%. Considerando que solo han sido necesarios los primeros 4 `epochs`, claramente se aprecia el beneficio de usar modelos pre-entrenados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de razas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la detección de razas será necesario cambiar el target y la estructura del modelo. Siguiendo una estrategia similar a la utilizada para la clasificación de especies, se procede a codificar las etiquetas mediante un `LabelEncoder`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_encoder = LabelEncoder()\n",
    "labels[\"encoded_breed\"] = breed_encoder.fit_transform(labels[\"breed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al evitar de la aumentación de datos, es necesario recalcular la media y la desviación estandar para la transformación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(CONFIG.get(\"IMAGE_SIZE\")),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*calculate_mean_std(labels[\"image_name\"])),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, al no usar aumento de datos, se usará el dataset original. Por ende, se creará un nuevo `split` de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_breed_data, train_breed_data, rest_breed_pred, train_breed_pred = train_test_split(\n",
    "    labels[\"image_name\"].values,\n",
    "    labels[\"encoded_breed\"].values,\n",
    "    test_size=0.7,\n",
    "    random_state=CONFIG.get(\"SEED\"),\n",
    ")\n",
    "\n",
    "val_breed_data, test_breed_data, val_breed_pred, test_breed_pred = train_test_split(\n",
    "    rest_breed_data, rest_breed_pred, test_size=0.5, random_state=CONFIG.get(\"SEED\")\n",
    ")\n",
    "\n",
    "breed_train_dl = DataLoader(\n",
    "    CatDogDataset(train_breed_data, train_breed_pred, breed_transform), batch_size=100\n",
    ")\n",
    "breed_val_dl = DataLoader(\n",
    "    CatDogDataset(val_breed_data, val_breed_pred, breed_transform), batch_size=100\n",
    ")\n",
    "breed_test_dl = DataLoader(\n",
    "    CatDogDataset(test_breed_data, test_breed_pred, breed_transform), batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, al necesitar ahora una salida por cada raza, debemos instanciar los modelos con `num_classes = 37`, que representa la cantidad de diferentes razas en el conjunto de datos actual. Por razones académicas, se ha optado por utilizar [ResNet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html), aprovechando sus pesos originales. Se ha modificado la última capa, reemplazándola con la estructura previamente indicada, para adaptarla al nuevo objetivo de detección de razas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogResnet50(37), input_size=(1, 3, 128, 128), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos el modelo con los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = CatDogResnet50(37)\n",
    "\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    resnet50_model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\"))\n",
    "    )\n",
    "else:\n",
    "    resnet50_model = resnet50_model.to(CONFIG.get(\"DEVICE\"))\n",
    "    resnet50_train_acc, resnet50_valid_acc, resnet50_train_loss, resnet50_valid_loss = train(\n",
    "        model=resnet50_model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=resnet50_model.parameters(),\n",
    "            lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\"),\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=breed_train_dl,\n",
    "        validation_loader=breed_val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        resnet50_model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(resnet50_train_acc, resnet50_valid_acc, \"Precisión\")\n",
    "    train_val_plot(resnet50_train_loss, resnet50_valid_loss, \"Perdida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por la grafica podemos apreciar que el modelo estuvo altamente acoplado hasta aproximadamente el `epoch` 6. A partir de ese punto, se empiezan a notar los indicios de `overfitting` o, alternativamente, el posible final del proceso de entrenamiento, dado que el conjunto de validación no muestra cambios significativos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(resnet50_model, breed_test_dl, CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "show_predictions(resnet50_model, breed_test_dl, breed_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de correlación, debido a su falta da valores númericos diferentes de 0, se podría considerar una [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix); proporcionando los resultados esperados con una precisión aceptable. Adicionalmente podemos observar y comparar la cantidad de parametros de resnet50 con la creada manualmente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(param.numel() for param in resnet50_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impresionantemente, resnet50 contiene 10,534,301 menos parametros. Esto nos indica que la arquitectura del modelo creado a mano es significativamente inferior; creando más merito a las personas que diseñan dichas redes neuronales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Se han usado y desarrollado diferentes redes neuronales convolucionales (CNN) para solventar la identificación de gatos o perros en diferentes imagenes y su posible raza.\n",
    "\n",
    "### Identificación de perros o gatos\n",
    "\n",
    "Para esta tarea, debido a un desbalance de datos, se ha optado por genear datos sinteticos mediante la aplicacion de transormaciones a los ya presentes. Especificamente, se generaron imágenes de gatos hasta llegar al mismo número de imágenes de perros\n",
    "\n",
    "Seguidamente, se ha creado una CNN a mano con aproximadamente 35M de parametros y modificado el modelo de resnet34 sustituyendo su ultima capa para adaptarlo a nuestro problema de clasificación binaria. Aunque el modelo creado a mano nos proporciona más maniobrabilidad, al diseño y los pesos de la red ya entredana de resnet34 ofrecen resultados de mayor calidad con menos `epoch` de entrenamiento.\n",
    "\n",
    "### Identificación de raza\n",
    "\n",
    "Debido a la distribución balanceada de los datos, no fue necesario aplicar un pre-procesamiento de los datos de entrada.\n",
    "\n",
    "Para la solución de esta tarea se ha optado por el uso de la red pre-entrenada Resnet50; modificando su ultima capa para adaptarse a el problema de clasificación de 37 clases. En solo 15 `epoch`, pudo obtener una precision del 70% en el conjunto de validación; corroborando la gran capacidad de abstracción de nuevas caracteristicas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
