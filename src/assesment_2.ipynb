{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2\n",
    "\n",
    "_Autores_\n",
    "\n",
    "-   Palmer Pérez, Rubén\n",
    "\n",
    "## Descipción de la practica\n",
    "\n",
    "Este proyecto se basa en el análisis del conjunto de datos [The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), abordando diversas problemáticas utilizando [Convolutional Neural Networks (CNN)](<(https://en.wikipedia.org/wiki/Convolutional_neural_network)>).\n",
    "\n",
    "### Tareas asignadas\n",
    "\n",
    "1. Clasificación Perros vs. Gatos:\n",
    "\n",
    "    - Implementación de clasificación para distinguir entre perros y gatos.\n",
    "    - Utilización de una red creada manualmente y otra preentrenada.\n",
    "\n",
    "2. Clasificación de la Raza:\n",
    "\n",
    "    - Clasificar la raza de los animales presentes en el conjunto de datos.\n",
    "\n",
    "3. Detección de la Posición de la Cabeza:\n",
    "\n",
    "    - Investigación y aplicación de técnicas para identificar la posición de la cabeza en las imágenes.\n",
    "\n",
    "4. Segmentación del Animal:\n",
    "\n",
    "    - Implementación de un sistema para segmentar el objeto animal en las imágenes proporcionadas.\n",
    "\n",
    "En este documento se examina el proceso de aprendizaje, procesamiento de datos y pruebas que han llevado a la solución de los dos primeros puntos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paquetes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from os import path, listdir\n",
    "from torchvision import transforms\n",
    "from torchview import draw_graph\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage import filters, img_as_ubyte, color, img_as_float\n",
    "from skimage.transform import rotate\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.util import random_noise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from functools import reduce\n",
    "from operator import sub\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from CatDogDataset import CatDogDataset\n",
    "from network import CatDogNet, CatDogResnet34, CatDogResnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"DEVICE\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"IMAGE_SIZE\": (128, 128),\n",
    "    \"DATA_PATH\": path.join(path.abspath(path.pardir), \"data\", \"images\"),\n",
    "    \"SEED\": 27,\n",
    "    \"MODEL\": {\"EPOCH\": 15, \"LEARNING_RATE\": 0.00001},\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "\n",
    "random.seed(CONFIG.get(\"SEED\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, eliminaremos posibles artefactos de previas ejecuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_remove = (\n",
    "    os.path.join(folder, file)\n",
    "    for folder, _, files in os.walk(path.join(CONFIG.get(\"DATA_PATH\")))\n",
    "    for file in files\n",
    "    if \"generated\" in file\n",
    ")\n",
    "\n",
    "any(map(os.remove, files_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "El conjunto de datos establece lo siguiente en los documentos adjuntos de etiquetado\n",
    "\n",
    "_Todas las imágenes con la 1ª letra en mayúscula son imágenes de gatos, mientras que las imágenes con la primera letra en minúscula son imágenes de perros._\n",
    "\n",
    "Gracias a que el nombre del archivo tambien indica la raza del animal, utilizaremos el nombre de cada imagen para generar un `dataframe` que contenga el `path` de cada imagen y las etiquetas necesarias. Adicionalmente comentar que, aunque se podría directamente codificar las etiquetas para su futuro uso, se mantendrá el uso de etiquetas con una mayor carga semántica; evitando los [números mágicos](<https://en.wikipedia.org/wiki/Magic_number_(programming)>) fuera del estudio de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [\n",
    "    path.join(CONFIG.get(\"DATA_PATH\"), file)\n",
    "    for file in listdir(CONFIG.get(\"DATA_PATH\"))\n",
    "    if file.endswith(\".jpg\")\n",
    "]\n",
    "\n",
    "labels = pd.DataFrame(\n",
    "    {\n",
    "        \"image_name\": image_files,\n",
    "        \"species\": [\n",
    "            \"cat\" if path.basename(file)[0].isupper() else \"dog\" for file in image_files\n",
    "        ],\n",
    "        \"breed\": [\n",
    "            \"_\".join(path.basename(file).split(\"_\")[:-1]).lower()\n",
    "            for file in image_files\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentar que, explorando los diferentes tipos de imagenes manualmente, se han encontrado diferentes extensiones. Con el objetivo de simplificar el proceso de procesamiento de imagenes, solo se han tenido en cuenta las imagenes con extensión `.jpg`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis\n",
    "\n",
    "Antes de cargar las imágenes, se examinarán las etiquetas y las imágenes para ver si existen patrones, desbalances en las clasificaciones, etc. Principalmente, nos interesa ver la distribución general de las etiquetas para comprobar si están equilibradas, ya que un desequilibrio podría afectar al rendimiento de los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    labels[\"species\"].values,\n",
    ")\n",
    "plt.title(\"Species frequency distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar un claro desbalance de las especies, viendo que hay casi el _doble_ de perros que gatos. En estos casos, se nos presentan tres posibilidades:\n",
    "\n",
    "-   Permanecer en este estado: El modelo podría predecir erroneamente unicamente por la cantidad de datos. Por ejemplo, en un caso extremo, si tuviéramos 1 gato y 99 perros, el modelo etiquetaría todas las imágenes como perros, ya que tiene un 99% de probabilidades de acertar.\n",
    "-   **Downsample**: Eliminar aleatoriamente observaciones de los perros hasta llegar al mismo numero de gatos; posiblemente reduciendo la precisión de acierto ante las predicciones del os perros\n",
    "-   **Upsample**: Usar [aumento de datos](https://en.wikipedia.org/wiki/Data_augmentation) para generar nuevos gatos hasta llegar a la cantida de perros; posiblemente creando `overfitting`\n",
    "\n",
    "En este especifico caso, y con objetivo academico, se ha decidido generar imagenes de gatos\n",
    "\n",
    "Tambien podemos observar la distribución de las razas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=labels, x=\"breed\", hue=\"species\")\n",
    "plt.title(\"Breed frequency distribution\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su distribución es relativamente estable, excepto por `stanffordshire_bull_terrier` y `scotish_terrier` que tienen ligeramente menos datos, por lo que no será necesario modificar los datos con respecto a la especie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gatos VS Perros: Clasificación\n",
    "\n",
    "### Aumento de datos\n",
    "\n",
    "Como ya se ha mencionado previamente, la falta de observaciones de gatos crea un desbalance de fequencia. Para reducir dicho impacto, se generarán nuevos gatos aplicando un conjunto aleatorio de transformaciones. Al final del proceso pues, deberíamos obtener un conjunto de imagenes de gatos adicionales que, idealmente, sean suficientemente fierentes como para que el modelo no sufra de `overfitting` o al menos se pueda reducir su efecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulations = [\n",
    "    lambda img: rotate(img, random.randint(0, 360), mode=\"wrap\"),\n",
    "    lambda img: random_noise(img, seed=CONFIG.get(\"SEED\"), mode=\"s&p\", amount=0.1),\n",
    "    lambda img: random_noise(img, seed=CONFIG.get(\"SEED\"), mode=\"gaussian\", var=0.01),\n",
    "    lambda img: img[::-1, ::-1],\n",
    "    lambda img: filters.gaussian(img, sigma=1),\n",
    "]\n",
    "\n",
    "PROPORTIONAL_MANIPULATION_AMOUNT = 2\n",
    "\n",
    "cat_data = labels[labels[\"species\"] == \"cat\"][\"image_name\"].to_list()\n",
    "\n",
    "amount_of_cats = abs(sub(*labels[\"species\"].value_counts().values))\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(amount_of_cats):\n",
    "    if CONFIG.get(\"DEBUG\"):\n",
    "        continue\n",
    "\n",
    "    random_manipulations = random.sample(\n",
    "        manipulations,\n",
    "        random.randint(1, len(manipulations) // PROPORTIONAL_MANIPULATION_AMOUNT),\n",
    "    )\n",
    "\n",
    "    choice = random.choice(cat_data)\n",
    "    img = reduce(\n",
    "        lambda img, manipulation: manipulation(img),\n",
    "        random_manipulations,\n",
    "        imread(choice),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        imsave(\n",
    "            path.join(path.join(CONFIG.get(\"DATA_PATH\")), f\"generated{i}.jpg\"),\n",
    "            img_as_ubyte(img),\n",
    "        )\n",
    "    except:\n",
    "        print(\"Had some error with image\", choice, \"skiping...\")\n",
    "        continue\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"image_name\": path.join(\n",
    "                path.join(CONFIG.get(\"DATA_PATH\")), f\"generated{i}.jpg\"\n",
    "            ),\n",
    "            \"species\": \"cat\",\n",
    "            \"breed\": \"_\".join(path.basename(choice).split(\"_\")[:-1]).lower(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "labels_cat_dogs = pd.concat([labels, pd.DataFrame(records)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    labels_cat_dogs[\"species\"].values,\n",
    ")\n",
    "plt.title(\"Species frequency distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pues, tenemos un conjunto de datos balanceados con respecto a la especie de los animales.\n",
    "\n",
    "### `Finetunning` de la media y la desviación estandar\n",
    "\n",
    "Al aplicar las transformaciones, se suele normalizar los datos de entrada para reducir la posible contribución de errores por valores extremos. Para ello, sse suelen usar los siguientes valores:\n",
    "\n",
    "-   media: [0.485, 0.456, 0.406]\n",
    "-   std: [0.229, 0.224, 0.225]\n",
    "\n",
    "Que son los los obtenidos por [ImageNet](https://image-net.org/). Sin embargo, y por razones académicas, se calcularán dichos valores especificamente para nuestro conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(paths: list[str]) -> tuple[int, int]:\n",
    "    mean = np.zeros(3)\n",
    "    std = np.zeros(3)\n",
    "\n",
    "    for path in paths:\n",
    "        try:\n",
    "            img = imread(path)\n",
    "            img = img_as_float(img)\n",
    "            if len(img.shape) == 2 or (len(img.shape) == 3 and img.shape[2] == 1):\n",
    "                img = color.gray2rgb(img)\n",
    "            img = img.transpose((2, 0, 1))\n",
    "            img = img.reshape(3, -1)\n",
    "            mean += img.mean(axis=1)\n",
    "            std += img.std(axis=1)\n",
    "        except:\n",
    "            print(\"failure on image\", path, \" skiping...\")\n",
    "            continue\n",
    "\n",
    "    mean /= len(paths)\n",
    "    std /= len(paths)\n",
    "\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "\n",
    "if CONFIG.get(\"DEBUG\"):\n",
    "    norm_data = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "else:\n",
    "    norm_data = calculate_mean_std(labels_cat_dogs[\"image_name\"])\n",
    "norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados, siendo extremadamente parecidos a los de ImageNet, nos reafirman que estamos cerca del dominio de resultados para imagenes genericas y que estos valores no son más que una especialización para este entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(CONFIG.get(\"IMAGE_SIZE\")),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*norm_data),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de las etiquetas\n",
    "\n",
    "Como ya se mencionó previamante, la etiquetas que actualmente poseemos están cargadas semánticamente. Sin embargo, para ofrecerlas como `target` para el modelo, será necesaria codificarlas numericamente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_encoder = LabelEncoder()\n",
    "labels_cat_dogs[\"encoded_species\"] = species_encoder.fit_transform(\n",
    "    labels_cat_dogs[\"species\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento, validación y prueba\n",
    "\n",
    "En este caso usaremos la división 70/15/15 encontrados [aquí](https://www.v7labs.com/blog/train-validation-test-set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_data, train_data, rest_pred, train_pred = train_test_split(\n",
    "    labels_cat_dogs[\"image_name\"].values,\n",
    "    labels_cat_dogs[\"encoded_species\"].values,\n",
    "    test_size=0.7,\n",
    "    random_state=CONFIG.get(\"SEED\"),\n",
    ")\n",
    "\n",
    "val_data, test_data, val_pred, test_pred = train_test_split(\n",
    "    rest_data, rest_pred, test_size=0.5, random_state=CONFIG.get(\"SEED\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, se ha creado una implementación personalizada basada en el modulo [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) de Pytorch, permitiendonos un uso sencillo e intuitivo de los `DataLoader`, encargados de ofrecer los datos al modelo aplicando las transformaciones especificadas previamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(CatDogDataset(train_data, train_pred, transform), batch_size=100)\n",
    "val_dl = DataLoader(CatDogDataset(val_data, val_pred, transform), batch_size=100)\n",
    "test_dl = DataLoader(CatDogDataset(test_data, test_pred, transform), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Especificación del modelo\n",
    "\n",
    "La CNN diseñada tiene como objetivo la clasificación binaria entre gatos y perros. Para ello se ha implementado la siguiente arquitectura:\n",
    "\n",
    "-   **Bloque convolucional**: un conjunto de bloques que aplican `Convolution2D`, `batch normalization`, `ReLU` y `MaxPool2D`. tiene como objetivo encargarse de aplicr dichas convoluciones reduciendo al maximo el ruido y los parametros\n",
    "-   **Dropout**: Para reducir la posibilidad de `overfitting` con el tradeback de posiblemente no llegar a su maximo nivel de predicción\n",
    "-   **1Dimensional**: Un conjunto de capas que, un vez transformado la matriz en un vector, aplican capas lineales\n",
    "\n",
    "A continuación se podrá ver la representación grafica de la arquitectura usando [torchview](https://github.com/mert-kurttutan/torchview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogNet(2), input_size=(1, 3, 128, 128), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Finalmente, entrenaremos el modelo un total de 15 epochs. Adicionalmente, y con el objetivo de apreciar si el modelo ha sufrido de overfitting, se visualizará las `time series` del `accuracy` del entrenamiento y la validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs, train_loader, validation_loader):\n",
    "    model.train()\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for data, label in train_loader:\n",
    "            data = data.to(CONFIG.get(\"DEVICE\"))\n",
    "            label = label.to(CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_accuracy += acc / len(train_loader)\n",
    "            epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        train_acc.append(epoch_accuracy)\n",
    "        print(\n",
    "            \"Epoch : {}, train accuracy : {}, train loss : {}\".format(\n",
    "                epoch + 1, epoch_accuracy, epoch_loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in validation_loader:\n",
    "                data = data.to(CONFIG.get(\"DEVICE\"))\n",
    "                label = label.to(CONFIG.get(\"DEVICE\"))\n",
    "\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "                # No optimizer as we don't want to change the weights\n",
    "\n",
    "                acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "                epoch_val_accuracy += acc / len(validation_loader)\n",
    "                epoch_val_loss += val_loss / len(validation_loader)\n",
    "\n",
    "            valid_acc.append(epoch_val_accuracy)\n",
    "            print(\n",
    "                \"Epoch : {}, val_accuracy : {}, val_loss : {}\".format(\n",
    "                    epoch + 1, epoch_val_accuracy, epoch_val_loss\n",
    "                )\n",
    "            )\n",
    "    model.eval()\n",
    "    return list(map(float, train_acc)), list(map(float, valid_acc))\n",
    "\n",
    "\n",
    "def train_val_plot(train, val):\n",
    "    plt.plot(range(len(train)), train, label=\"Entrenamiento\")\n",
    "    plt.plot(range(len(val)), val, label=\"Validación\")\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\n",
    "        \"Accuracy del conjunto de entrenamiento y validación con respecto al tiempo\"\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = CatDogNet(2)\n",
    "\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\"))\n",
    "    )\n",
    "else:\n",
    "    model = model.to(CONFIG.get(\"DEVICE\"))\n",
    "    train_acc, valid_acc = train(\n",
    "        model=model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=model.parameters(), lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\")\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=train_dl,\n",
    "        validation_loader=val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(train_acc, valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como, con el tiempo, el conjunto de validación va perdiendo poco a poco el incremento de `accuracy`. Un aumento de su conjunto de datos podría mejorar dicho aumento. Importantemente, podemos apreciar como el modelo no sufre de un `overfitting` especialmente agresivo. A continuación, mediante el uso del conjunto de test, se evaluará cuan buenas son las predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    overall_acc = 0\n",
    "    for image, label in test_loader:\n",
    "        image, label = [data.to(device) for data in (image, label)]\n",
    "\n",
    "        test_output = model(image)\n",
    "        acc = float((test_output.argmax(dim=1) == label).float().mean())\n",
    "        overall_acc += acc / len(test_dl)\n",
    "\n",
    "    return overall_acc\n",
    "\n",
    "\n",
    "test(model, test_dl, CONFIG.get(\"DEVICE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos observar como el modelo ha podido generalizar la idea de `gato` o `perro`, ya que la `accuracy` obtenida es, relativamente, alta. Adicionalmente, podemos representar graficamante, para que el lector pueda apreciar, un conjunto de imagenes con sus respectivas predicciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, data_loader, label_encoder, n_images=4):\n",
    "    test_cat_dog_pairs = list(zip(*next(iter(data_loader))))\n",
    "\n",
    "    random_images = random.sample(test_cat_dog_pairs, n_images)\n",
    "\n",
    "    _, axs = plt.subplots(1, n_images, figsize=(15, 10))\n",
    "\n",
    "    for i, (image, label) in enumerate(random_images):\n",
    "        prediction = model(torch.unsqueeze(image, 0)).argmax(dim=1)\n",
    "        image = image.detach().cpu().numpy()\n",
    "\n",
    "        if image.shape[0] == 1:\n",
    "            image = np.repeat(image, 3, axis=0)\n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "        axs[i].imshow(np.transpose(image, (1, 2, 0)), vmin=0, vmax=1)\n",
    "        axs[i].set_title(\n",
    "            f\"Real: {label_encoder.inverse_transform([label])[0]}\\nPrediction: {label_encoder.inverse_transform(prediction)[0]}\"\n",
    "        )\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_predictions(model, test_dl, species_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet34\n",
    "\n",
    "Se usara el modelo [Resnet34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html) para comparar los anteriores resultados. Se entrenará con los mismos datos, transformaciones, criterio y optimizador por lo que, esencialmente, solo estamos cambiando el modelo usado. Para solucionar el problema de dimensionalidad del modelo original, se ha eliminado la ultima capa y remplazado por la siguiente estructura:\n",
    "\n",
    "-   Flatten()\n",
    "-   Linear(512, 512)\n",
    "-   ReLU()\n",
    "-   Dropout(0.5)\n",
    "-   Linear(512, num_classes)\n",
    "\n",
    "En este caso el `num_classes` es 2 ya que es un problema de clasificación binaria. Esta pues es la estructura final:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogResnet34(2), input_size=(1, 3, 256, 256), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos el modelo. Comentar que, debido a que usamos los pesos originales de resnet34, el proceso de entrenamiento será mucho más rápido de lo esperado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34_model = CatDogResnet34(2)\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    resnet34_model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\"))\n",
    "    )\n",
    "else:\n",
    "    resnet34_model = resnet34_model.to(CONFIG.get(\"DEVICE\"))\n",
    "    resnet34_train_acc, resnet34_valid_acc = train(\n",
    "        model=resnet34_model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=resnet34_model.parameters(),\n",
    "            lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\"),\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=train_dl,\n",
    "        validation_loader=val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        resnet34_model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet34_model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(resnet34_train_acc, resnet34_valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REPLACE**\n",
    "\n",
    "Por los logs podemos apreciar como el modelo ha llegado al 99% de `acuracy` en el modelo de entrenamiento aun que solo un 60% en el de validación y graficamente podemos apreciar el proceso de aprendizaje de resnet. Curiosamente el conjunto de validación no ha subido sustancialmente durante este proceso; posiblemente generado por un `overfitting` del conjunto de entrenamiento o la falta dedatos con respecto al conjunto de validación. Para responder a esta incertidumbre visualizaremos un conjunto de imagenes, como previamente se ha ejecutado, con la etiqueta real y su predicción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(resnet34_model, test_dl, CONFIG.get(\"DEVICE\")))\n",
    "\n",
    "show_predictions(resnet34_model, test_dl, species_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene una `accuracy` del 63%, aumentando las sospechas que el modelo ha sufrido de `overfitting`, ya sea por una falta de datos en el conjunto de validación o por la arquitectura del modelo.\n",
    "\n",
    "Finalmente podemos concluir que, bajo los datos usados, optimizador y transformaciones aplicadas, el modelo presonalizado ofrece mejores resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de razas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la detección de razas será necesario cambiar el target y la estructura del modelo. Inicialmente, y al igual que con la especies, se codifican las etiquetas que serán el `target` del entrenamiento mediante un `LabelEncoder`. Esto nos permitirá en un futuro invertir la transformación facilmente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_encoder = LabelEncoder()\n",
    "labels[\"encoded_breed\"] = breed_encoder.fit_transform(labels[\"breed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al evitar de la aumentación de datos, es necesario recalcular la media y la desviación estandar para la transformación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(CONFIG.get(\"IMAGE_SIZE\")),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(*calculate_mean_std(labels[\"image_name\"])),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, al no usar aumento de datos, se usará el dataset original. Por ende, se creará un nuevo `split` de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_breed_data, train_breed_data, rest_breed_pred, train_breed_pred = train_test_split(\n",
    "    labels[\"image_name\"].values,\n",
    "    labels[\"encoded_breed\"].values,\n",
    "    test_size=0.7,\n",
    "    random_state=CONFIG.get(\"SEED\"),\n",
    ")\n",
    "\n",
    "val_breed_data, test_breed_data, val_breed_pred, test_breed_pred = train_test_split(\n",
    "    rest_breed_data, rest_breed_pred, test_size=0.5, random_state=CONFIG.get(\"SEED\")\n",
    ")\n",
    "\n",
    "breed_train_dl = DataLoader(\n",
    "    CatDogDataset(train_breed_data, train_breed_pred, breed_transform), batch_size=100\n",
    ")\n",
    "breed_val_dl = DataLoader(\n",
    "    CatDogDataset(val_breed_data, val_breed_pred, breed_transform), batch_size=100\n",
    ")\n",
    "breed_test_dl = DataLoader(\n",
    "    CatDogDataset(test_breed_data, test_breed_pred, breed_transform), batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, al necesitar ahora una salida por cada raza, debemos instanciar los modelos con `num_classes = 37`, que es la cantidad de diferentes razas que poseemos actualmente en el conjunto de datos. Comentar que, por interés académico, se ha decidido usar [Resnet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html); con el uso de sus pesos originales y cambiando la ultima capa por la estructura previamente mostrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(CatDogResnet50(37), input_size=(1, 3, 128, 128), device=\"meta\").visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos el modelo con los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = CatDogResnet50(37)\n",
    "\n",
    "if path.exists(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\")):\n",
    "    print(\"Found model state dict, loading ...\")\n",
    "    resnet50_model.load_state_dict(\n",
    "        torch.load(path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\"))\n",
    "    )\n",
    "else:\n",
    "    resnet50_model = resnet50_model.to(CONFIG.get(\"DEVICE\"))\n",
    "    resnet50_train_acc, resnet50_valid_acc = train(\n",
    "        model=resnet50_model,\n",
    "        optimizer=optim.Adam(\n",
    "            params=resnet50_model.parameters(),\n",
    "            lr=CONFIG.get(\"MODEL\").get(\"LEARNING_RATE\"),\n",
    "        ),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        epochs=CONFIG.get(\"MODEL\").get(\"EPOCH\"),\n",
    "        train_loader=breed_train_dl,\n",
    "        validation_loader=breed_val_dl,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        resnet50_model.state_dict(),\n",
    "        path.join(path.dirname(CONFIG.get(\"DATA_PATH\")), \"resnet50_model\"),\n",
    "    )\n",
    "\n",
    "    train_val_plot(resnet50_train_acc, resnet50_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test(resnet50_model, breed_test_dl, CONFIG.get(\"DEVICE\")))\n",
    "\n",
    "show_predictions(resnet50_model, breed_test_dl, breed_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una visión mas exhaustiva, se visualizará la matriz de correlación para todo el conjunto de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_predictions = []\n",
    "resnet50_real_labels = []\n",
    "for images, labels in breed_test_dl:\n",
    "    images, labels = [data.to(CONFIG.get(\"DEVICE\")) for data in (images, labels)]\n",
    "\n",
    "    resnet50_predictions.extend(list(map(int, resnet50_model(images).argmax(dim=1))))\n",
    "    resnet50_real_labels.extend(list(map(int, labels)))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    breed_encoder.inverse_transform(resnet50_real_labels),\n",
    "    breed_encoder.inverse_transform(resnet50_predictions),\n",
    ")\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(13, 13))\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
